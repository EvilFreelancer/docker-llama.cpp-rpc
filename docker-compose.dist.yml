version: "3.9"

x-shared-logs: &shared-logs
  logging:
    driver: "json-file"
    options:
      max-size: "10k"

x-shared-deploy: &shared-deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [ gpu ]
  <<: *shared-logs

services:

  main:
    image: evilfreelancer/llama.cpp-rpc:latest
    restart: "unless-stopped"
    #build:
    #  context: ./llama.cpp
    #  args:
    #    - LLAMACPP_VERSION=b3700
    volumes:
      - ./models:/app/models
    environment:
      # Режим работы (RPC-клиент в формате API-сервера)
      APP_MODE: server
      # Путь до весов, предварительно загруженной модели, внутри контейнера
      APP_MODEL: /app/models/TinyLlama-1.1B-q4_0.gguf
      # Адреса RPC-серверов с которыми будет взаимодействовать клиент
      APP_RPC_BACKENDS: backend-cuda:50052,backend-cpu:50052
    ports:
      - "127.0.0.1:8080:8080"
    <<: *shared-logs

  backend-cpu:
    image: evilfreelancer/llama.cpp-rpc:latest
    restart: "unless-stopped"
    #build:
    #  context: ./llama.cpp
    #  args:
    #    - LLAMACPP_VERSION=b3700
    environment:
      # Режим работы (RPC-сервер)
      APP_MODE: backend
      # Количество доступной RPC-серверу системной оперативной памяти (в Мегабайтах)
      APP_MEM: 2048
    ports:
      - "127.0.0.1:50152:50052"
    <<: *shared-logs

  backend-cuda:
    image: evilfreelancer/llama.cpp-rpc:latest-cuda
    restart: "unless-stopped"
    #build:
    #  context: ./llama.cpp
    #  dockerfile: Dockerfile.cuda
    #  args:
    #    - LLAMACPP_VERSION=b3700
    environment:
      # Режим работы (RPC-сервер)
      APP_MODE: backend
      # Количество доступной RPC-серверу оперативной памяти видеокарты (в Мегабайтах)
      APP_MEM: 1024
    ports:
      - "127.0.0.1:50252:50052"
    <<: *shared-deploy
