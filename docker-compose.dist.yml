version: "3.9"

x-shared-logs: &shared-logs
  logging:
    driver: "json-file"
    options:
      max-size: "10k"

x-shared-deploy: &shared-deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [ gpu ]
  <<: *shared-logs

services:

  main:
    image: evilfreelancer/llama.cpp-rpc:latest
    restart: "unless-stopped"
    build:
      context: ./llama.cpp
      args:
        - LLAMACPP_VERSION=b3600
    volumes:
      - ./models:/app/models
    environment:
      # Режим RPC клиента
      MODE: none
    ports:
      - "127.0.0.1:8888:8080"
    <<: *shared-logs

  backend-cpu:
    image: evilfreelancer/llama.cpp-rpc:latest
    restart: "unless-stopped"
    #build:
    #  context: ./llama.cpp
    #  args:
    #    - LLAMACPP_VERSION=b3700
    environment:
      # Режим RPC бэкенда
      MODE: backend
    ports:
      - "127.0.0.1:50152:50052"
    <<: *shared-logs

  backend-cuda:
    image: evilfreelancer/llama.cpp-rpc:latest-cuda
    restart: "unless-stopped"
    #build:
    #  context: ./llama.cpp
    #  dockerfile: Dockerfile.cuda
    #  args:
    #    - LLAMACPP_VERSION=b3700
    environment:
      # Вы можете указать, какое CUDA-устройство использовать
      CUDA_VISIBLE_DEVICES: 0
      # Режим RPC бэкенда
      MODE: backend
    ports:
      - "127.0.0.1:50252:50052"
    <<: *shared-deploy
